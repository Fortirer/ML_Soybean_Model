# -*- coding: utf-8 -*-
"""regressao2024_SojaModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YYWyOD6Ahs9Htl8NRUiXENKsMdDELGSS

# Machine Learning aplicado à Saude


---


# Parte I - REGRESSÃO

Bibliotecas adicionais:

- dfply: facilitador para manipulação de dataframes. https://github.com/kieferk/dfply

- yellowbrick: visualização de modelos preditivos. https://www.scikit-yb.org/en/latest/
"""

!pip install dfply >> /dev/null
!pip install yellowbrick >> /dev/null

# >> /dev/null para suprimir saída

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd # para processamento de bancos de dados
import numpy as np # para processamento numérico de bancos de dados
from dfply import *  # para importar os comandos da biblioteca dfply
import matplotlib.pyplot as plt # para geração de gráficos
from matplotlib import rc  # configurações adicionais para os gráficos a serem gerados

# informamos ao Python que estamos usando um notebook e que os gráficos devem ser exibidos nele
# %matplotlib inline
import seaborn as sns #alternativa para a matplotlib para geração de gráficos

# definimos o estilo dos gráficos
# mais estilos em https://matplotlib.org/3.1.1/gallery/#style-sheets
plt.style.use("fivethirtyeight")
# %config InlineBackend.figure_format = 'retina' # formato das imagens
rc('font',**{'family':'sans-serif','sans-serif':['DejaVu Sans'],'size':10}) #fonte utilizada
rc('mathtext',**{'default':'regular'})

import warnings   # ignorando os warnings emitidos pelo Python
warnings.filterwarnings("ignore")

import operator  # para ordenação do zip

np.random.seed(42)  # semente de aleatoriedade

from sklearn.model_selection import train_test_split  # importamos a funcionalidade de split do conjunto de dados em treino/teste

from sklearn.metrics import mean_squared_error, r2_score  # métricas de performance para modelos de regressão

"""### Importando o conjunto de dados como um Pandas DataFrame"""

# o conjunto de dados está no Google Drive, em um CSV separado por ;
banco = pd.read_csv('https://drive.google.com/uc?export=download&id=1y6ESHpXVMd15a5aiJ4hVs412vmZmk_EV', sep = ';')

# verificando as características básicas do conjunto de dados
banco.info()

"""### Seleção de municípios com mais de 10.000 habitantes"""

# usamos os comandos mask e select da biblioteca dfply para filtrar o conjunto de dados
banco_filtrado = banco >> mask(X.PopResid > 10000) >> select(~X.PopResid)

# para evitarmos a exibição dos dados em notação científica
pd.set_option('display.float_format', lambda x: '%.3f' % x)

# estatísticas básicas do conjuntos de dados
banco_filtrado.describe()

"""### Separar conjunto de dados em treinamento e teste"""

# variável de interesse
outcome = banco_filtrado >> select(X.ExpecVida)

# removemos a coluna ExpecVida e cod_municipio do conjunto de dados (axis = 1 representa coluna. Inplace é para atualizar a variável)
banco_filtrado.drop(['ExpecVida', 'cod_municipio'], axis = 1, inplace = True)

# fazemos a separação do conjunto de dados em treino/teste, com 30% dos dados para teste
X_train, X_test, y_train, y_test = train_test_split(banco_filtrado, outcome, test_size=0.3)

X_train.shape #quantidade de registros para treino (linhas, colunas)

X_test.shape #quantidade de registros para teste (linhas, colunas)

"""### Pré-processamento dos dados de treinamento

#### Variáveis quantitativas
"""

# verificando a correlação entre as variáveis

# método de Pearson
corr=X_train.corr(method='pearson')
plt.figure(figsize=(15, 15))
sns.heatmap(corr, vmax=.8, linewidths=0.01,
          square=True,annot=True,cmap='YlGnBu',linecolor="white")
plt.title('Correlation between features')
plt.show()

"""### Filtrar preditores com variância nula"""

# importamos a função VarianceThreshold
from sklearn.feature_selection import VarianceThreshold

# Escolha o limiar aceitável de variância
threshold = 0

selector = VarianceThreshold(threshold)
selector.fit_transform(X_train)
for i,s in enumerate(selector.get_support()):
  if s:
    print(X_train.columns[i] + " - manter " + "["+ str(selector.variances_[i]) + "]")
  else:
    print("*** " + X_train.columns[i] + " - remover " + "["+ str(selector.variances_[i]) + "]")

# verificar features constantes

constant_features = [
    feat for feat in X_train.columns if X_train[feat].std() == 0
]

constant_features

"""### Padronizar os dados de treino e teste"""

# importamos a funcao StandardScaler para padronização dos dados
from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

# obtemos as estatísticas necessárias para a padronização a partir do conjunto de treinamento
sc.fit(X_train)

sc.mean_

sc.var_

# padronizamos o conjunto de treinamento
X_train = pd.DataFrame(sc.transform(X_train), columns=X_train.columns)

# padronizamos o conjunto de teste
X_test = pd.DataFrame(sc.transform(X_test), columns=X_test.columns)

X_train.head()

X_test.head()

"""#### Random Forest Regressor


Scikit Random Forest Regressor: https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html
"""

from sklearn.ensemble import RandomForestRegressor

"""Grid Search vs. Random Search

![alt text](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ2DP4D5htDWBdhRUlhC6tbGuWIEK2wJo2mNcWIRfcT8LH7UhIH)
"""

from sklearn.model_selection import RandomizedSearchCV, GridSearchCV

# modelo random forest
rf = RandomForestRegressor(random_state=42)

"""Hiperparâmetros a serem otimizados"""

# Número de árvores no Random Forest
n_estimators = [int(x) for x in np.linspace(start = 200, stop = 500, num = 10)]
# Número de features a serem consideradas a cada split
max_features = ['auto', 'sqrt']
# Número máximo de níveis na árvore
max_depth = [int(x) for x in np.linspace(5, 30, num = 5)]
# Número mínimo de amostras necessárias para dividir um nó
min_samples_split = [2, 5, 10]
# Número mínimo de amostras necessárias em cada leaf node
min_samples_leaf = [1, 2, 4]
# Método de seleção das amostras para treinamento de cada árvore
bootstrap = [True]


# Criação do param grid
param_grid_rf = {'n_estimators': n_estimators,
               'max_features': max_features,
               'max_depth': max_depth,
               'min_samples_split': min_samples_split,
               'min_samples_leaf': min_samples_leaf,
               'bootstrap': bootstrap}

# otimizando os hiperparâmetros
rf_random = RandomizedSearchCV(estimator = rf,
                               param_distributions = param_grid_rf,
                               n_iter = 50, ### número de avaliações do Random Grid
                               random_state = 42,
                               cv = 3,
                               verbose = 2,
                               n_jobs = -1)

# executando a busca
rf_random = rf_random.fit(X_train, y_train)

# resultados da busca
print('Melhor score: %s' % rf_random.best_score_)
print('Melhores hiperparâmetros: %s' % rf_random.best_params_)

# selecionando apenas os resultados da busca
rf = RandomForestRegressor(n_estimators = 2000,
                           min_samples_split = 2,
                           min_samples_leaf = 1,
                           max_features = 'sqrt',
                           max_depth = 30,
                           bootstrap = False,
                           random_state=42)

# treinando o modelo otimizado
rf.fit(X_train, y_train)

# medidas de performance
pred_train_rf= rf.predict(X_train)
print('RMSE (treino):', np.sqrt(mean_squared_error(y_train,pred_train_rf)))
print('R² (treino):', r2_score(y_train, pred_train_rf))

pred_test_rf= rf.predict(X_test)
print('RMSE (teste):',np.sqrt(mean_squared_error(y_test,pred_test_rf)))
print('R² (teste):', r2_score(y_test, pred_test_rf))

# gráfico de dispersão: valor real vs valor predito
plt.style.use('seaborn')
fig = plt.figure()
plt.figure(figsize = (10, 10))
plt.scatter(y_test, pred_test_rf, alpha=0.5, marker='o', color='blue', linewidth=1)
plt.title("Gráfico de Dispersão")
plt.xlabel('Valor Real')
plt.ylabel('Valor Predito')
plt.show()

# importância de variáveis com SHAP
!pip install shap >> /dev/null
import shap

explainer = shap.Explainer(rf, X_test)
shap_values = explainer(X_test)
shap.summary_plot(shap_values, X_test)

# seleção de variáveis com boruta
! pip install boruta >> /dev/null
from boruta import BorutaPy

# inicializa o Random Forest (max_depth de 3 a 7 segundo o git do Boruta)
rf_boruta = RandomForestRegressor(n_jobs=-1, random_state=42, max_depth=5)

# define o Boruta como método de seleção de variáveis
boruta = BorutaPy(estimator = rf_boruta, n_estimators = 'auto', verbose = 2, random_state = 42, max_iter = 100)

# fit Boruta
boruta.fit(np.array(banco_filtrado), np.array(outcome))

# mostrando os resultados
area_verde = X.columns[boruta.support_].to_list()
area_azul = X.columns[boruta.support_weak_].to_list()

"""O boruta não selecionou nenhuma variável como inconclusiva ou rejeitada.

---
"""